A scalable algorithm that can detect fine grained repetitions quickly across large spiking datasets is desirable, as such a frame work would provides a means to test for the tendency of neuronal activity to revisit states. 

Quickly identifying repeated states in large scale neuronal data and simulation is important, as the degree of repitition should influence the mindset of the scientists undertaking the analysis. For instance many theoretical Neuroscientists believe assumptions that valid cortical neuronal simulations should consist of Asynchronous Irregular activity (AI), and they may often simulate such activity using @brunel1996hebbian Brunel's balanced model of cortex. However, new data sets prominently capture replayed states, and previously collected of spike trains may have been misleading, as too their limited ability to capture replay, and detect it in analysis.

Although the dynamic systems view of the brain is old, a survey of spiking datasets which can detect and labels network attractor states in large spike count data is merited, as this would bolster the dynamic systems view of the neuronal learning. 

By quantifying repetitions large spiking datasets, using geometric representations of complex spike patterns, we can quantify the frequency of repitition, and achieve a better understanding of a networks ability to revisit states. To this end we represented time bound neural activity as simple geometric coordinates in a highdimensional space. Working with geometric representations of chaotic spike train recordings may enable researchers to interrogate the state-fullness of both biologically recorded spike trains and their digitally simulated counterparts. Furthermore, there is reason to believe that when mammal brains enact visual object recognition encoded memories guide cortical neurons to “replay” previously observed neural states, as replayed spiking states may cohere with the visual brains perceptual recognition of a familiar scene.